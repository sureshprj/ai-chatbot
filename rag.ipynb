{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8366a22d",
   "metadata": {},
   "source": [
    "### Loading a text file (basic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e75733ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "## load a single text file\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "\n",
    "loader = TextLoader(\"rag_data/leave_policy.txt\", encoding=\"utf-8\")\n",
    "\n",
    "documents = loader.load()\n",
    "print(f\" Loaded {len(documents)} file\")\n",
    "print(f\"{documents[0].metadata}\")\n",
    "print(f\"First 50 chars...{documents[0].page_content[:50]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5433325c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## load a directory \n",
    "## load all text file inside the rag_data directory\n",
    "\n",
    "from langchain_community.document_loaders import DirectoryLoader, TextLoader\n",
    "\n",
    "dir_loader = DirectoryLoader(\n",
    "    \"rag_data\",\n",
    "    glob= \"**/*.txt\",\n",
    "    loader_cls = TextLoader,\n",
    "    loader_kwargs={'encoding': 'utf-8'},\n",
    "    show_progress= True\n",
    ")\n",
    "\n",
    "loaded_files = dir_loader.load()\n",
    "print(f\"loaded files lengh: {len(loaded_files)}\")\n",
    "\n",
    "for i, doc in enumerate(loaded_files):\n",
    "    print(f\" meta data: {doc.metadata}\")\n",
    "    print(f\" \\n page content: {doc.page_content[:10]} ....\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db78bb32",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Text spliting techniques\n",
    "\n",
    "from langchain_text_splitters import CharacterTextSplitter, RecursiveCharacterTextSplitter, TokenTextSplitter\n",
    "text_content = documents[0].page_content\n",
    "\n",
    "### Character text Splitter\n",
    "char_splitter = CharacterTextSplitter(\n",
    "    separator = \"\\n\",  # split based on new lines\n",
    "    chunk_size = 200, # max chunk size characters\n",
    "    chunk_overlap= 20, # overlap between chunks\n",
    ")\n",
    "chunks_list = char_splitter.split_text(text_content)\n",
    "print(f\"******charactor text splitting**************\")\n",
    "print(f\"chunk size {len(chunks_list)}\")\n",
    "print(f\"first chunk:: {chunks_list[0]}\")\n",
    "\n",
    "\n",
    "### Recursive text splitter\n",
    "recursive_chr_splitter = RecursiveCharacterTextSplitter(\n",
    "    separators = [\"\\n\\n\", \"\\n\", \" \", \"\"],\n",
    "    chunk_size = 200,\n",
    "    chunk_overlap = 20, \n",
    ")\n",
    "chunks_list = recursive_chr_splitter.split_text(text_content)\n",
    "print(f\"******Recursive text splitting**************\")\n",
    "print(f\"chunk size {len(chunks_list)}\")\n",
    "print(f\"first chunk:: {chunks_list[0]}\")\n",
    "\n",
    "### Token based text splitter\n",
    "token_chr_splitter = TokenTextSplitter(\n",
    "    chunk_size = 100,\n",
    "    chunk_overlap = 20\n",
    ")\n",
    "chunks_list = token_chr_splitter.split_text(text_content)\n",
    "print(f\"******Token text splitting**************\")\n",
    "print(f\"chunk size {len(chunks_list)}\")\n",
    "print(f\"first chunk:: {chunks_list[0]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bcd8290",
   "metadata": {},
   "source": [
    "### PDF file loader \n",
    "\n",
    "It's same like text file loader only but we may face isssues with pdf file because pdf means not only text it may have images, diagrams, and some empty spaces so best practice is we need to do cleanup.\n",
    "\n",
    "below code will load pdf file using pyPDF loader and split the text content using recursive character."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcf12a5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## PDF spliting statergies\n",
    "from typing import List\n",
    "from langchain_community.document_loaders import (\n",
    "    PyPDFLoader,\n",
    "    PyMuPDFLoader,\n",
    "    UnstructuredPDFLoader\n",
    ")\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_core.documents import Document # Document type\n",
    "\n",
    "print (\"### pdf loading using pydfLoader\")\n",
    "\n",
    "class PDFprocessor:\n",
    "    ### best way to processing the pdf file \n",
    "    def __init__(self, chunk_size=200, chunk_overlap=100, separators = [\" \"]):\n",
    "        self.chunk_size = chunk_size\n",
    "        self.chunk_overlap=chunk_overlap\n",
    "        self.text_splitter = RecursiveCharacterTextSplitter(\n",
    "            chunk_size = chunk_size,\n",
    "            chunk_overlap = chunk_overlap,\n",
    "            separators= separators\n",
    "        )\n",
    "    def _clean_text(self, text: str)-> str:\n",
    "        # remove extra spaces\n",
    "        text = \" \".join(text.split())\n",
    "        return text\n",
    "\n",
    "    def process_pdf(self, pdf_path)->List[Document]:\n",
    "        #load pdf file\n",
    "        loader = PyPDFLoader(pdf_path)\n",
    "        pdf_pages = loader.load()\n",
    "        processed_chunks = []\n",
    "        ## cleanup each pages before split\n",
    "        for page_num, page in enumerate(pdf_pages):\n",
    "            cleaned_text = self._clean_text(page.page_content)\n",
    "            chunks = self.text_splitter.create_documents(\n",
    "                texts = [cleaned_text],\n",
    "                metadatas= [\n",
    "                    {\n",
    "                        **page.metadata,\n",
    "                        \"page\": page_num + 1,\n",
    "                        \"total_pages\": len(pdf_pages),\n",
    "                        \"chunk_method\": \"pdf_processor\"\n",
    "                    }\n",
    "                ]\n",
    "            )\n",
    "            processed_chunks.extend(chunks)\n",
    "\n",
    "        return processed_chunks\n",
    "\n",
    "try:\n",
    "    preprocess = PDFprocessor()\n",
    "    output_chunks = preprocess.process_pdf(\"rag_data/work_timing_policy.pdf\")\n",
    "    print(f\"chunks {len(output_chunks)} created\")\n",
    "    print(f\"{output_chunks[0].page_content}\")\n",
    "except Exception as e:\n",
    "    print(f\"error:: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d959f2f",
   "metadata": {},
   "source": [
    "### Word document processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90d94ef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import Docx2txtLoader, UnstructuredWordDocumentLoader\n",
    "\n",
    "print(\"Using Docx2txtLoader\")\n",
    "\n",
    "try:\n",
    "    docx_loader = Docx2txtLoader(\"rag_data/frontend.docx\")\n",
    "    docs = docx_loader.load()\n",
    "    print(f\"Loaded {len(docs)} documents\")\n",
    "    print(f\"page content {docs[0].page_content[:100]}...\")\n",
    "except Exception as e:\n",
    "    print(f\"error:: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da4a920e",
   "metadata": {},
   "source": [
    "### CSV and exeel files - sturcture data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f454b4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import CSVLoader\n",
    "\n",
    "csv_loader = CSVLoader( \n",
    "    file_path='rag_data/dividend.csv',\n",
    "    encoding='utf-8',\n",
    "    csv_args= {\n",
    "        'delimiter': ',',\n",
    "        'quotechar':  '\"'\n",
    "    }\n",
    ")\n",
    "\n",
    "csv_docs = csv_loader.load()\n",
    "print(f\"loaded {len(csv_docs)} rows\") ## each row will be a one doc\n",
    "print(f\"\\n First row:\")\n",
    "print(f'\\n content: {csv_docs[2].page_content}')\n",
    "print(f'\\n content: {csv_docs[2].metadata}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33c78044",
   "metadata": {},
   "source": [
    "### JSON parsking and processing\n",
    "\n",
    "we may get json result from an api so we should know know how to process it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e23bc517",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from langchain_community.document_loaders import JSONLoader\n",
    "\n",
    "json_loader = JSONLoader(\n",
    "    file_path = 'rag_data/employee.json',\n",
    "    jq_schema= \".[]\", # use jq to extract items (if your file contains a list)\n",
    "    text_content = False\n",
    ")\n",
    "\n",
    "json = json_loader.load()\n",
    "\n",
    "print(f\"data loaded {json[0].page_content}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f375163",
   "metadata": {},
   "source": [
    "### Database loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7acce1cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "## create simple SQLite database and popultate test data\n",
    "\n",
    "import sqlite3\n",
    "import os\n",
    "\n",
    "# Database file path\n",
    "db_file = \"rag_data/company.db\"\n",
    "\n",
    "# If DB exists, delete it to start fresh\n",
    "if os.path.exists(db_file):\n",
    "    os.remove(db_file)\n",
    "    print(\"Existing database removed.\")\n",
    "\n",
    "# Create new SQLite connection\n",
    "conn = sqlite3.connect(db_file)\n",
    "cursor = conn.cursor()\n",
    "\n",
    "# Create tables\n",
    "cursor.execute('''\n",
    "CREATE TABLE IF NOT EXISTS employees (\n",
    "    id INTEGER PRIMARY KEY,\n",
    "    name TEXT,\n",
    "    role TEXT,\n",
    "    department TEXT,\n",
    "    salary REAL\n",
    ")\n",
    "''')\n",
    "\n",
    "cursor.execute('''\n",
    "CREATE TABLE IF NOT EXISTS projects (\n",
    "    id INTEGER PRIMARY KEY,\n",
    "    name TEXT,\n",
    "    status TEXT,\n",
    "    budget REAL,\n",
    "    lead_id INTEGER,\n",
    "    FOREIGN KEY (lead_id) REFERENCES employees(id)\n",
    ")\n",
    "''')\n",
    "\n",
    "# Insert dummy employee data\n",
    "employees = [\n",
    "    (1, \"Alice\", \"Developer\", \"Engineering\", 90000),\n",
    "    (2, \"Bob\", \"Manager\", \"Sales\", 105000),\n",
    "    (3, \"Charlie\", \"Analyst\", \"Finance\", 80000),\n",
    "    (4, \"Diana\", \"Designer\", \"UI/UX\", 85000),\n",
    "]\n",
    "\n",
    "cursor.executemany('''\n",
    "INSERT INTO employees (id, name, role, department, salary)\n",
    "VALUES (?, ?, ?, ?, ?)\n",
    "''', employees)\n",
    "\n",
    "# Insert dummy project data\n",
    "projects = [\n",
    "    (1, \"Project Alpha\", \"Active\", 250000, 2),\n",
    "    (2, \"Project Beta\", \"Planning\", 150000, 1),\n",
    "    (3, \"Project Gamma\", \"Completed\", 300000, 3),\n",
    "]\n",
    "\n",
    "cursor.executemany('''\n",
    "INSERT INTO projects (id, name, status, budget, lead_id)\n",
    "VALUES (?, ?, ?, ?, ?)\n",
    "''', projects)\n",
    "\n",
    "cursor.execute(\"select * from employees\")\n",
    "print(cursor.fetchall()[0])\n",
    "# Commit and close connection\n",
    "conn.commit()\n",
    "conn.close()\n",
    "\n",
    "print(\"Database created and populated successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e4a3e0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "from typing import List\n",
    "from langchain_core.documents import Document\n",
    "db_file = \"rag_data/company.db\"\n",
    "def SQL_to_documents(db_path: str) -> List[Document]:\n",
    "    \"\"\"\n",
    "    Reads all tables in an SQLite database and converts them into a list of LangChain Document objects.\n",
    "    Each Document represents one table, containing all its rows as text.\n",
    "    \"\"\"\n",
    "    documents = []\n",
    "    conn = sqlite3.connect(db_path)\n",
    "    cursor = conn.cursor()\n",
    "\n",
    "    # Get all table names\n",
    "    cursor.execute(\"SELECT name FROM sqlite_master WHERE type='table';\")\n",
    "    tables = [row[0] for row in cursor.fetchall()]\n",
    "\n",
    "    for table in tables:\n",
    "        # Fetch all data from the table\n",
    "        cursor.execute(f\"SELECT * FROM {table};\")\n",
    "        rows = cursor.fetchall()\n",
    "        columns = [desc[0] for desc in cursor.description]\n",
    "\n",
    "        # Combine all rows into text\n",
    "        table_data = \"\\n\".join(\n",
    "            [\", \".join(f\"{col}={val}\" for col, val in zip(columns, row)) for row in rows]\n",
    "        )\n",
    "\n",
    "        # Create a Document for the table\n",
    "        doc = Document(\n",
    "            page_content=f\"Table: {table}\\n{table_data}\",\n",
    "            metadata={\"table_name\": table, \"columns\": columns, \"row_count\": len(rows)}\n",
    "        )\n",
    "        documents.append(doc)\n",
    "\n",
    "    conn.close()\n",
    "    return documents\n",
    "\n",
    "print(\"DATABASE DATA\")\n",
    "print(SQL_to_documents(db_file))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fe2deb5",
   "metadata": {},
   "source": [
    "### Embeddings\n",
    "\n",
    "- in short we are taking the text and convert into embeddings(numberical represation)\n",
    "- based on search (cosine similarity search) it give the score of matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1b11925",
   "metadata": {},
   "outputs": [],
   "source": [
    "## simple example for embeddings\n",
    "import numpy as np \n",
    "# simple 3D example  (real embeddings have 100+ Dimensions)\n",
    "# Let's take movies search, with 3 axes (Action, Comedy, Suspense)\n",
    "# Each axis value represents the intensity/score of Action, Comedy, Suspense\n",
    "\n",
    "word_embeddings = {\n",
    "    \"Iron_Man\": [0.8, 0.3, 0.2],\n",
    "    \"Hulk\": [0.9, 0.3, 0.2],\n",
    "    \"Shawshank_Redemption\": [0.2, 0.3, 0.9]\n",
    "}\n",
    "\n",
    "# Similarity check\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Extract vectors\n",
    "vec1 = word_embeddings[\"Iron_Man\"]\n",
    "vec2 = word_embeddings[\"Hulk\"]\n",
    "vec3 = word_embeddings[\"Shawshank_Redemption\"]\n",
    "\n",
    "# Compute pairwise cosine similarities\n",
    "sim_iron_hulk = cosine_similarity([vec1], [vec2])[0][0]\n",
    "sim_iron_shawshank = cosine_similarity([vec1], [vec3])[0][0]\n",
    "sim_hulk_shawshank = cosine_similarity([vec2], [vec3])[0][0]\n",
    "\n",
    "# Print results\n",
    "print(f\"Similarity (Iron_Man vs Hulk): {sim_iron_hulk:.3f}\")\n",
    "print(f\"Similarity (Iron_Man vs Shawshank_Redemption): {sim_iron_shawshank:.3f}\")\n",
    "print(f\"Similarity (Hulk vs Shawshank_Redemption): {sim_hulk_shawshank:.3f}\")\n",
    "\n",
    "#We represent each movie as a 3D vector:\n",
    "#Axis 1 → Action\n",
    "#Axis 2 → Comedy\n",
    "#Axis 3 → Suspense\n",
    "#cosine_similarity measures how directionally similar two vectors are (values between -1 and 1).\n",
    "# close 1 means it matching, -1 close to -1 not matching\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aed3f7d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#let use real models to convert the text into embeddings\n",
    "\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "embeddings = HuggingFaceEmbeddings(\n",
    "    model_name = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    ")\n",
    "\n",
    "text = \"hello, i am learning embeedings\"\n",
    "embed = embeddings.embed_query(text) # query means single line \n",
    "# embeds = embeddings.embed_documents([..]) # list of string call documents\n",
    "print(f\" Text: {text}\")\n",
    "print(f\"Embedding length: {len(embed)}\") #384 is static lenght because this model using 384D\n",
    "print(embed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44fc0aab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function four cosine similrarity search\n",
    "def cosine_similarity(vec1, vec2):\n",
    "    \"\"\"\n",
    "    Calculate the cosine similarity between two vectors.\n",
    "\n",
    "    Cosine similarity measures the angle between two vectors in a multi-dimensional space.\n",
    "    - A value close to 1 → vectors point in the same direction (very similar)\n",
    "    - A value close to 0 → vectors are orthogonal (not related)\n",
    "    - A value close to -1 → vectors point in opposite directions (opposite meanings)\n",
    "    \"\"\"\n",
    "\n",
    "    # Step 1: Compute the dot product of the two vectors\n",
    "    # The dot product measures how much two vectors point in the same direction.\n",
    "    dot_product = np.dot(vec1, vec2)\n",
    "\n",
    "    # Step 2: Compute the L2 norm (magnitude) of each vector\n",
    "    # The norm represents the length (magnitude) of a vector.\n",
    "    # np.linalg.norm() calculates the square root of the sum of squared vector elements.\n",
    "    norm_a = np.linalg.norm(vec1)\n",
    "    norm_b = np.linalg.norm(vec2)\n",
    "\n",
    "    # Step 3: Calculate cosine similarity\n",
    "    # Formula: cos(θ) = (A · B) / (||A|| * ||B||) \n",
    "    # This gives the cosine of the angle θ between the two vectors.\n",
    "    cosine_sim = dot_product / (norm_a * norm_b)\n",
    "\n",
    "    # Step 4: Return the similarity score\n",
    "    return cosine_sim\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "379e1cfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#example docs and query \n",
    "\n",
    "documents = [\n",
    "    \"LangChain is a framework used to build applications powered by large language models.\",\n",
    "    \"Python is a high-level programming language widely used for automation and data processing.\",\n",
    "    \"Machine learning is a field of artificial intelligence that focuses on training models from data.\",\n",
    "    \"Embeddings convert text into numerical vectors so semantic similarity can be measured.\",\n",
    "    \"The weather is sunny today with clear skies and warm temperatures.\"\n",
    "]\n",
    "\n",
    "question = \"Is it raining today?\"\n",
    "\n",
    "# creating a method for semantic search\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "embeddings = HuggingFaceEmbeddings(\n",
    "    model_name = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    ")\n",
    "\n",
    "def semantic_search(query, documents, top_k=3):\n",
    "    #embed query and document\n",
    "    query_embedding = embeddings.embed_query(query) # this will convert into the embedding (vector numbers)\n",
    "    query_docs  = embeddings.embed_documents( documents) # this will convert into the list of embeddings\n",
    "\n",
    "    #calculate the similartiy score\n",
    "    similiraties = []\n",
    "\n",
    "    for i,doc in enumerate(query_docs):\n",
    "        sim = cosine_similarity(query_embedding, doc)\n",
    "        similiraties.append((sim, documents[i]))\n",
    "\n",
    "    # sort by similiraties\n",
    "    similiraties.sort(reverse=True)\n",
    "    return similiraties[:top_k]\n",
    "print(f\"Question: {question}\")\n",
    "print(f\"matching results: ${semantic_search(question, documents, 2)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1113c62",
   "metadata": {},
   "source": [
    "### Store the embeddings into a vector store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32a2bef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.documents import Document\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "# your raw texts\n",
    "documents = [\n",
    "    \"LangChain is a framework used to build applications powered by large language models.\",\n",
    "    \"Python is a high-level programming language widely used for automation and data processing.\",\n",
    "    \"Machine learning is a field of artificial intelligence that focuses on training models from data.\",\n",
    "    \"Embeddings convert text into numerical vectors so semantic similarity can be measured.\",\n",
    "    \"The weather is sunny today with clear skies and warm temperatures.\"\n",
    "]\n",
    "\n",
    "# create DocumentList\n",
    "doc_list = [Document(page_content=text) for text in documents]\n",
    "\n",
    "# choose an embedding model (simple + local)\n",
    "embedding_model = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "# store in Chroma\n",
    "vector_store = Chroma.from_documents(\n",
    "    documents=doc_list,\n",
    "    embedding=embedding_model,\n",
    "    collection_name=\"info_docs\",\n",
    "    persist_directory=\"./chrom_db\"\n",
    ")\n",
    "\n",
    "print(\"Stored successfully!\")\n",
    "question = \"what is large language models\"\n",
    "print(\"SEarch question:\"+ question)\n",
    "print(\"*******RESULT*********\")\n",
    "print(vector_store.similarity_search(question, 3))\n",
    "print(vector_store.similarity_search_with_score(question, 3))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5e94e67",
   "metadata": {},
   "source": [
    "### Vector Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3808e5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_pinecone import PineconeVectorStore\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from pinecone import Pinecone, ServerlessSpec\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "def save_into_vector(doc_list):\n",
    "    # 1. Create embeddings\n",
    "    embeddings = HuggingFaceEmbeddings(\n",
    "        model_name=\"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "    )\n",
    "\n",
    "    # 2. Initialize Pinecone client\n",
    "    pc = Pinecone(api_key=os.environ[\"PINECONE_API_KEY\"])\n",
    "\n",
    "    index_name = \"leave-policy-index\" # eq to DB name\n",
    "\n",
    "    # 3. Create the Pinecone index if it doesn’t exist\n",
    "    if index_name not in pc.list_indexes().names():\n",
    "        pc.create_index(\n",
    "            name=index_name,\n",
    "            dimension=384,  # MiniLM-L6-v2 embedding dimension\n",
    "            metric=\"cosine\",\n",
    "            spec=ServerlessSpec(\n",
    "                cloud=\"aws\",\n",
    "                region=\"us-east-1\"\n",
    "            )\n",
    "        )\n",
    "\n",
    "    # 4. Connect to the index\n",
    "    index = pc.Index(index_name)\n",
    "\n",
    "    # 5. Create vector store (stores docs into Pinecone)\n",
    "    vector_store = PineconeVectorStore.from_documents(\n",
    "        documents=doc_list,\n",
    "        embedding=embeddings,\n",
    "        index_name=index_name\n",
    "    )\n",
    "\n",
    "    print(\"Documents stored in Pinecone\")\n",
    "    return vector_store\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# your raw texts\n",
    "documents = [\n",
    "    \"LangChain is a framework used to build applications powered by large language models.\",\n",
    "    \"Python is a high-level programming language widely used for automation and data processing.\",\n",
    "    \"Machine learning is a field of artificial intelligence that focuses on training models from data.\",\n",
    "    \"Embeddings convert text into numerical vectors so semantic similarity can be measured.\",\n",
    "    \"The weather is sunny today with clear skies and warm temperatures.\"\n",
    "]\n",
    "\n",
    "# create DocumentList\n",
    "doc_list = [Document(page_content=text) for text in documents]\n",
    "\n",
    "vector_DB = save_into_vector(doc_list)\n",
    "\n",
    "output = vector_DB.similarity_search(\"LLM framework\", k=2)\n",
    "\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b89afbd4",
   "metadata": {},
   "source": [
    "### Semantic chunking"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
